{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine learning process\n",
    "\n",
    "- [Import data](#Import-data)\n",
    "- [Test some algorithms](#Test-some-algorithms)\n",
    "    - [First test - all values](#First-test---all-values)\n",
    "    - [Removing outliers by distance using IQR](#Removing-gps-points-outliers--by-distance-using-IQR)\n",
    "    - [Removing outliers by distance using Z-Score](#Removing-outliers-by-distance-using-Z-Score)\n",
    "    - [Removing outliers by distance using Z-Score modified](#Removing-outliers-by-distance-using-Z-Score-modified)\n",
    "- [GridSearchCV with Random Forest and Gradient Boost](#GridSearchCV-with-Random-Forest-and-Gradient-Boost)\n",
    "    - [Random forest](#Random-forest)\n",
    "    - [Gradient boost](#Gradient-boost)\n",
    "- [Train all data with Random forest](#Train-all-data-with-Random-forest)\n",
    "    - [Test (~2.56 km)](#Test)\n",
    "- [Ploting results](#Ploting-results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import ast\n",
    "import datetime\n",
    "import holidays\n",
    "import pickle\n",
    "from math import radians, cos, sin, asin, sqrt\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.svm import LinearSVR\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import make_scorer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trata_dados(df):\n",
    "    #--- Timestamp\n",
    "    df['month'] = df['timestamp'].dt.month\n",
    "    df['day'] = df['timestamp'].dt.day\n",
    "    #df['cat_hour'] = pd.cut(df.timestamp.dt.hour, [-1, 8, 16, 23], labels=[0,1,2])\n",
    "    df['hour'] = df.timestamp.dt.hour\n",
    "    df['fds_fer'] = df.timestamp.dt.dayofweek\n",
    "    df['fds_fer'] = df['fds_fer'].apply(lambda x: 1 if x==0 or x==6 else 0) #dom/sab = 0\n",
    "    for data in sorted(holidays.Portugal(years=[2013, 2014])): # add holidays\n",
    "        df['fds_fer'] = 1 if len(df[df.timestamp.dt.date==data])>0 else 0\n",
    "    \n",
    "    del df['timestamp']\n",
    "\n",
    "    #--- Call_type hot encode\n",
    "    df['call_type'] = preprocessing.LabelEncoder().fit_transform(df['call_type'])\n",
    "    enc = OneHotEncoder().fit_transform(df[['call_type']])\n",
    "    del df['call_type']\n",
    "    df['call_type_a'] = enc.toarray()[:,0]\n",
    "    df['call_type_b'] = enc.toarray()[:,1]\n",
    "    df['call_type_c'] = enc.toarray()[:,2]\n",
    "\n",
    "    #--- Fill na\n",
    "    df.fillna(-1, inplace=True)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def carrega_dados(dataset='', n_rows=None, test_df=False):\n",
    "    try:\n",
    "        if test_df==False:\n",
    "            usecols = ['CALL_TYPE', 'ORIGIN_CALL', 'ORIGIN_STAND', 'TAXI_ID', 'TIMESTAMP',\n",
    "                       'dist_perc', 'start_lat', 'start_lon', 'stop_lat', 'stop_lon']\n",
    "        else:\n",
    "            usecols = ['TRIP_ID','CALL_TYPE', 'ORIGIN_CALL', 'ORIGIN_STAND', 'TAXI_ID', 'TIMESTAMP',\n",
    "                       'dist_perc', 'start_lat', 'start_lon', 'stop_lat', 'stop_lon']\n",
    "\n",
    "        if n_rows is None:\n",
    "            df = pd.read_csv(dataset, usecols=usecols, parse_dates=True)\n",
    "        else:\n",
    "            df = pd.read_csv(dataset, usecols=usecols, parse_dates=True, nrows=n_rows)\n",
    "\n",
    "        # header in lower case\n",
    "        df.columns = df.columns.map(lambda x: x.lower())\n",
    "        #converting obj do timestamp\n",
    "        df.timestamp = pd.to_datetime(df.timestamp)\n",
    "\n",
    "        return trata_dados(df)\n",
    "    except Exception as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = carrega_dados('train_tratado_outliers_2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>origin_call</th>\n",
       "      <th>origin_stand</th>\n",
       "      <th>taxi_id</th>\n",
       "      <th>dist_perc</th>\n",
       "      <th>start_lat</th>\n",
       "      <th>start_lon</th>\n",
       "      <th>stop_lat</th>\n",
       "      <th>stop_lon</th>\n",
       "      <th>month</th>\n",
       "      <th>day</th>\n",
       "      <th>hour</th>\n",
       "      <th>fds_fer</th>\n",
       "      <th>call_type_a</th>\n",
       "      <th>call_type_b</th>\n",
       "      <th>call_type_c</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>20000589</td>\n",
       "      <td>2.23</td>\n",
       "      <td>41.141412</td>\n",
       "      <td>-8.618643</td>\n",
       "      <td>41.154489</td>\n",
       "      <td>-8.630838</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>20000596</td>\n",
       "      <td>3.46</td>\n",
       "      <td>41.159826</td>\n",
       "      <td>-8.639847</td>\n",
       "      <td>41.170671</td>\n",
       "      <td>-8.665740</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>20000320</td>\n",
       "      <td>17.63</td>\n",
       "      <td>41.140359</td>\n",
       "      <td>-8.612964</td>\n",
       "      <td>41.140530</td>\n",
       "      <td>-8.615970</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>20000520</td>\n",
       "      <td>7.97</td>\n",
       "      <td>41.151951</td>\n",
       "      <td>-8.574678</td>\n",
       "      <td>41.142915</td>\n",
       "      <td>-8.607996</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>20000337</td>\n",
       "      <td>4.81</td>\n",
       "      <td>41.180490</td>\n",
       "      <td>-8.645994</td>\n",
       "      <td>41.178087</td>\n",
       "      <td>-8.687268</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   origin_call  origin_stand   taxi_id  dist_perc  start_lat  start_lon  \\\n",
       "0         -1.0          -1.0  20000589       2.23  41.141412  -8.618643   \n",
       "1         -1.0           7.0  20000596       3.46  41.159826  -8.639847   \n",
       "2         -1.0          -1.0  20000320      17.63  41.140359  -8.612964   \n",
       "3         -1.0          -1.0  20000520       7.97  41.151951  -8.574678   \n",
       "4         -1.0          -1.0  20000337       4.81  41.180490  -8.645994   \n",
       "\n",
       "    stop_lat  stop_lon  month  day  hour  fds_fer  call_type_a  call_type_b  \\\n",
       "0  41.154489 -8.630838      7    1     0        0          0.0          0.0   \n",
       "1  41.170671 -8.665740      7    1     0        0          0.0          1.0   \n",
       "2  41.140530 -8.615970      7    1     0        0          0.0          0.0   \n",
       "3  41.142915 -8.607996      7    1     0        0          0.0          0.0   \n",
       "4  41.178087 -8.687268      7    1     0        0          0.0          0.0   \n",
       "\n",
       "   call_type_c  \n",
       "0          1.0  \n",
       "1          0.0  \n",
       "2          1.0  \n",
       "3          1.0  \n",
       "4          1.0  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test some algorithms\n",
    "\n",
    "Testing some algorithms performance (speed, precision, etc)..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Runs some algorithms for compare\n",
    "\n",
    "def algs_test(X1, X2, y1, y2):\n",
    "\n",
    "    algs =[('linear', LinearRegression()),\n",
    "           ('ridge', Ridge(random_state=99)),\n",
    "           ('lasso', Lasso(random_state=99)), #lasso 142.7065081365015 561.2795889474457\n",
    "           ('lsvr', LinearSVR(random_state=99)),\n",
    "           ('KNN reg', KNeighborsRegressor()), #KNN reg 3.3861972620770264 3.5182287757352873\n",
    "           ('gboost', GradientBoostingRegressor(random_state=99)),\n",
    "           ('ada boost', AdaBoostRegressor(random_state=99)),\n",
    "           ('rnd forest', RandomForestRegressor(random_state=99)),\n",
    "           ('xgboost', XGBRegressor(random_state=99, n_jobs=-1))\n",
    "           ]\n",
    "\n",
    "    for name, alg in algs:\n",
    "        try:\n",
    "            alg.fit(X1, y1)\n",
    "            preds = alg.predict(X2)\n",
    "\n",
    "            concat = np.apply_along_axis(np.radians, 1, np.concatenate((np.array(y2), preds), axis=1)) \n",
    "            hav_dist = np.apply_along_axis(lambda x: 6371*(2*np.arcsin(np.sqrt((np.sin((x[3]-x[1])/2)**2 + np.cos(x[1])*np.cos(x[3])*np.sin((x[2]-x[0])/2)**2)))), 1, concat)\n",
    "\n",
    "            print('normal',name, hav_dist.mean(), hav_dist.std())\n",
    "        except: # if algorithm dont have native support to multioutput\n",
    "            mo = MultiOutputRegressor(alg, n_jobs=-1).fit(X1, y1)\n",
    "            preds = mo.predict(X2)\n",
    "\n",
    "            concat = np.apply_along_axis(np.radians, 1, np.concatenate((np.array(y2), preds), axis=1)) \n",
    "            hav_dist = np.apply_along_axis(lambda x: 6371*(2*np.arcsin(np.sqrt((np.sin((x[3]-x[1])/2)**2 + np.cos(x[1])*np.cos(x[3])*np.sin((x[2]-x[0])/2)**2)))), 1, concat)\n",
    "\n",
    "            print('multi',name, hav_dist.mean(), hav_dist.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (excelent) outliers reference: http://colingorrie.github.io/outlier-detection.html\n",
    "\n",
    "def outliers_iqr(ys):\n",
    "    quartile_1, quartile_3 = np.percentile(ys, [25, 75])\n",
    "    iqr = quartile_3 - quartile_1\n",
    "    lower_bound = quartile_1 - (iqr * 1.5)\n",
    "    upper_bound = quartile_3 + (iqr * 1.5)\n",
    "    return np.where((ys > upper_bound) | (ys < lower_bound))\n",
    "\n",
    "def outliers_z_score(ys):\n",
    "    threshold = 3\n",
    "\n",
    "    mean_y = np.mean(ys)\n",
    "    print(mean_y)\n",
    "    stdev_y = np.std(ys)\n",
    "    z_scores = [(y - mean_y) / stdev_y for y in ys]\n",
    "    return np.where(np.abs(z_scores) > threshold)\n",
    "\n",
    "\n",
    "def outliers_modified_z_score(ys):\n",
    "    threshold = 3.5\n",
    "\n",
    "    median_y = np.median(ys)\n",
    "    median_absolute_deviation_y = np.median([np.abs(y - median_y) for y in ys])\n",
    "    modified_z_scores = [0.6745 * (y - median_y) / median_absolute_deviation_y\n",
    "                         for y in ys]\n",
    "    return np.where(np.abs(modified_z_scores) > threshold)\n",
    "\n",
    "#------------#\n",
    "# Train and test stay in same scale, because this, \n",
    "# scaler.transform is invoked in final's method only.\n",
    "\n",
    "def train_test_mimax(df, remove_outliers=''):\n",
    "    \n",
    "    y = df[['stop_lon', 'stop_lat']]\n",
    "    X = df.drop(['stop_lon', 'stop_lat'], axis=1)\n",
    "    scaler = MinMaxScaler().fit(X)\n",
    "\n",
    "    #-- split train/test #X1, X2, y1, y2\n",
    "    X1, X2, y1, y2 = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "    \n",
    "    if(remove_outliers=='iqr'):\n",
    "        idx = outliers_iqr(X1.dist_perc)[0].tolist() \n",
    "        X1 = X1[~X1.index.isin(idx)]\n",
    "        y1 = y1[~y1.index.isin(idx)]\n",
    "    \n",
    "    if(remove_outliers=='zscore'):\n",
    "        idx = outliers_z_score(X1.dist_perc)[0].tolist() \n",
    "        X1 = X1[~X1.index.isin(idx)]\n",
    "        y1 = y1[~y1.index.isin(idx)]\n",
    "    \n",
    "    if(remove_outliers=='zscore_mod'):\n",
    "        idx = outliers_modified_z_score(X1.dist_perc)[0].tolist() \n",
    "        X1 = X1[~X1.index.isin(idx)]\n",
    "        y1 = y1[~y1.index.isin(idx)]\n",
    "        \n",
    "    X1 = scaler.transform(X1)\n",
    "    X2 = scaler.transform(X2)\n",
    "    \n",
    "    return X1, X2, y1, y2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First test - all values\n",
    "\n",
    "Testing with outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "normal linear 3.242526950006376 4.572296738247557\n",
      "normal ridge 5.035208578741894 7.074214279575095\n",
      "normal lasso 36.324779071030704 303.92947601040646\n",
      "multi lsvr 3.1863218127999597 4.762258288342083\n",
      "normal KNN reg 3.4755950381935605 6.459272728794591\n",
      "multi gboost 2.7746041839015096 3.200689795563214\n",
      "multi ada boost 3.146199186825982 3.406473200539824\n",
      "normal rnd forest 2.5479666072422242 3.406634550586585\n",
      "multi xgboost 2.7918836267241756 3.2605204176922067\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <bound method DMatrix.__del__ of <xgboost.core.DMatrix object at 0x000000CF92D16438>>\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\a46396\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\core.py\", line 482, in __del__\n",
      "    if self.handle is not None:\n",
      "AttributeError: 'DMatrix' object has no attribute 'handle'\n"
     ]
    }
   ],
   "source": [
    "algs_test(*train_test_mimax(carrega_dados('train_tratado_outliers_2.csv', n_rows=100000)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing outliers  by distance using IQR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "normal linear 3.2425833774315493 4.575463792948822\n",
      "normal ridge 5.065930261976431 7.113446517186023\n",
      "normal lasso 36.4648121986655 303.9111462474312\n",
      "multi lsvr 3.1932358868652537 4.7679315770933\n",
      "normal KNN reg 3.512267713886255 11.501153300541784\n",
      "multi gboost 2.7720885351485074 3.192751752980376\n",
      "multi ada boost 3.178256252983925 3.4851434820065754\n",
      "normal rnd forest 2.5530415257189283 3.3677081978555594\n",
      "multi xgboost 2.7990510140260403 3.254266204733062\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <bound method DMatrix.__del__ of <xgboost.core.DMatrix object at 0x000000CF92CE5FD0>>\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\a46396\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\core.py\", line 482, in __del__\n",
      "    if self.handle is not None:\n",
      "AttributeError: 'DMatrix' object has no attribute 'handle'\n"
     ]
    }
   ],
   "source": [
    "algs_test(*train_test_mimax(carrega_dados('train_tratado_outliers_2.csv', n_rows=100000), 'iqr'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing outliers by distance using Z-Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.524522999999997\n",
      "normal linear 3.2436593040954613 4.574459667786016\n",
      "normal ridge 5.039750150854115 7.079425583180473\n",
      "normal lasso 36.28555882688959 303.93460837250666\n",
      "multi lsvr 3.1662636567340985 4.768181989539964\n",
      "normal KNN reg 3.507742216388956 11.5014849400449\n",
      "multi gboost 2.7784649464210047 3.215223893740343\n",
      "multi ada boost 8.09105263027827 6.628493068706677\n",
      "normal rnd forest 2.5466122160199474 3.3476191777671227\n",
      "multi xgboost 2.7912804981779225 3.227982029127283\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <bound method DMatrix.__del__ of <xgboost.core.DMatrix object at 0x000000CF945B3C88>>\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\a46396\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\core.py\", line 482, in __del__\n",
      "    if self.handle is not None:\n",
      "AttributeError: 'DMatrix' object has no attribute 'handle'\n"
     ]
    }
   ],
   "source": [
    "algs_test(*train_test_mimax(carrega_dados('train_tratado_outliers_2.csv', n_rows=100000), 'zscore'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing outliers by distance using Z-Score modified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "normal linear 3.242174052228836 4.573566803728244\n",
      "normal ridge 5.061994285006496 7.10780595989326\n",
      "normal lasso 36.48563984433075 303.90842226180456\n",
      "multi lsvr 3.1965189978395223 4.778043385957267\n",
      "normal KNN reg 3.5118850508358066 11.501055786030161\n",
      "multi gboost 2.7727059251229975 3.2011773458718835\n",
      "multi ada boost 8.673090286200152 5.762570196449435\n",
      "normal rnd forest 2.558399525354379 3.374796435729142\n",
      "multi xgboost 2.8103219355324685 3.2733977711563074\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <bound method DMatrix.__del__ of <xgboost.core.DMatrix object at 0x000000CFCC98C940>>\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\a46396\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xgboost\\core.py\", line 482, in __del__\n",
      "    if self.handle is not None:\n",
      "AttributeError: 'DMatrix' object has no attribute 'handle'\n"
     ]
    }
   ],
   "source": [
    "algs_test(*train_test_mimax(carrega_dados('train_tratado_outliers_2.csv', n_rows=100000), 'zscore_mod'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removing outliers and training multiple algorithms with default parameters did not make much difference.\n",
    "\n",
    "To have a greater variance, we will use the outliers and the ensemble algorithms (Gradient boost and Random Forest):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GridSearchCV with Random Forest and Gradient Boost\n",
    "\n",
    "Testing over 100k lines..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Haversine is the distance between 2 points in earth.\n",
    "This function is in sklearn.make_scorer format.\n",
    "\"\"\"\n",
    "\n",
    "# lon1, lat1, lon2, lat2\n",
    "#  0     1     2     3\n",
    "def haversine_scorer(y_real, y_pred):\n",
    "    concat = np.apply_along_axis(np.radians, 1, np.concatenate((np.array(y_real), y_pred), axis=1)) \n",
    "    return np.apply_along_axis(lambda x: 6371*(2*np.arcsin(np.sqrt((np.sin((x[3]-x[1])/2)**2 + np.cos(x[1])*np.cos(x[3])*np.sin((x[2]-x[0])/2)**2)))), 1, concat).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_ = make_scorer(haversine_scorer, greater_is_better=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df[['stop_lon', 'stop_lat']]\n",
    "X = df.drop(['stop_lon', 'stop_lat'], axis=1)\n",
    "X = MinMaxScaler().fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 4 candidates, totalling 40 fits\n",
      "[CV] max_depth=10, min_samples_leaf=50 ...............................\n",
      "[CV] ................ max_depth=10, min_samples_leaf=50, total=  41.7s\n",
      "[CV] max_depth=10, min_samples_leaf=50 ...............................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:   46.4s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ................ max_depth=10, min_samples_leaf=50, total=  41.3s\n",
      "[CV] max_depth=10, min_samples_leaf=50 ...............................\n",
      "[CV] ................ max_depth=10, min_samples_leaf=50, total=  41.3s\n",
      "[CV] max_depth=10, min_samples_leaf=50 ...............................\n",
      "[CV] ................ max_depth=10, min_samples_leaf=50, total=  41.2s\n",
      "[CV] max_depth=10, min_samples_leaf=50 ...............................\n",
      "[CV] ................ max_depth=10, min_samples_leaf=50, total=  41.2s\n",
      "[CV] max_depth=10, min_samples_leaf=50 ...............................\n",
      "[CV] ................ max_depth=10, min_samples_leaf=50, total=  42.1s\n",
      "[CV] max_depth=10, min_samples_leaf=50 ...............................\n",
      "[CV] ................ max_depth=10, min_samples_leaf=50, total=  41.8s\n",
      "[CV] max_depth=10, min_samples_leaf=50 ...............................\n",
      "[CV] ................ max_depth=10, min_samples_leaf=50, total=  41.6s\n",
      "[CV] max_depth=10, min_samples_leaf=50 ...............................\n",
      "[CV] ................ max_depth=10, min_samples_leaf=50, total=  41.3s\n",
      "[CV] max_depth=10, min_samples_leaf=50 ...............................\n",
      "[CV] ................ max_depth=10, min_samples_leaf=50, total=  43.7s\n",
      "[CV] max_depth=10, min_samples_leaf=150 ..............................\n",
      "[CV] ............... max_depth=10, min_samples_leaf=150, total=  40.4s\n",
      "[CV] max_depth=10, min_samples_leaf=150 ..............................\n",
      "[CV] ............... max_depth=10, min_samples_leaf=150, total=  40.1s\n",
      "[CV] max_depth=10, min_samples_leaf=150 ..............................\n",
      "[CV] ............... max_depth=10, min_samples_leaf=150, total=  40.1s\n",
      "[CV] max_depth=10, min_samples_leaf=150 ..............................\n",
      "[CV] ............... max_depth=10, min_samples_leaf=150, total=  40.3s\n",
      "[CV] max_depth=10, min_samples_leaf=150 ..............................\n",
      "[CV] ............... max_depth=10, min_samples_leaf=150, total=  40.4s\n",
      "[CV] max_depth=10, min_samples_leaf=150 ..............................\n",
      "[CV] ............... max_depth=10, min_samples_leaf=150, total=  40.2s\n",
      "[CV] max_depth=10, min_samples_leaf=150 ..............................\n",
      "[CV] ............... max_depth=10, min_samples_leaf=150, total=  40.0s\n",
      "[CV] max_depth=10, min_samples_leaf=150 ..............................\n",
      "[CV] ............... max_depth=10, min_samples_leaf=150, total=  40.3s\n",
      "[CV] max_depth=10, min_samples_leaf=150 ..............................\n",
      "[CV] ............... max_depth=10, min_samples_leaf=150, total=  40.5s\n",
      "[CV] max_depth=10, min_samples_leaf=150 ..............................\n",
      "[CV] ............... max_depth=10, min_samples_leaf=150, total=  40.6s\n",
      "[CV] max_depth=40, min_samples_leaf=50 ...............................\n",
      "[CV] ................ max_depth=40, min_samples_leaf=50, total=  53.5s\n",
      "[CV] max_depth=40, min_samples_leaf=50 ...............................\n",
      "[CV] ................ max_depth=40, min_samples_leaf=50, total=  53.7s\n",
      "[CV] max_depth=40, min_samples_leaf=50 ...............................\n",
      "[CV] ................ max_depth=40, min_samples_leaf=50, total=  53.7s\n",
      "[CV] max_depth=40, min_samples_leaf=50 ...............................\n",
      "[CV] ................ max_depth=40, min_samples_leaf=50, total=  54.9s\n",
      "[CV] max_depth=40, min_samples_leaf=50 ...............................\n",
      "[CV] ................ max_depth=40, min_samples_leaf=50, total=  53.8s\n",
      "[CV] max_depth=40, min_samples_leaf=50 ...............................\n",
      "[CV] ................ max_depth=40, min_samples_leaf=50, total=  54.1s\n",
      "[CV] max_depth=40, min_samples_leaf=50 ...............................\n",
      "[CV] ................ max_depth=40, min_samples_leaf=50, total=  53.6s\n",
      "[CV] max_depth=40, min_samples_leaf=50 ...............................\n",
      "[CV] ................ max_depth=40, min_samples_leaf=50, total=  53.8s\n",
      "[CV] max_depth=40, min_samples_leaf=50 ...............................\n",
      "[CV] ................ max_depth=40, min_samples_leaf=50, total=  53.9s\n",
      "[CV] max_depth=40, min_samples_leaf=50 ...............................\n",
      "[CV] ................ max_depth=40, min_samples_leaf=50, total=  53.8s\n",
      "[CV] max_depth=40, min_samples_leaf=150 ..............................\n",
      "[CV] ............... max_depth=40, min_samples_leaf=150, total=  46.2s\n",
      "[CV] max_depth=40, min_samples_leaf=150 ..............................\n",
      "[CV] ............... max_depth=40, min_samples_leaf=150, total=  46.1s\n",
      "[CV] max_depth=40, min_samples_leaf=150 ..............................\n",
      "[CV] ............... max_depth=40, min_samples_leaf=150, total=  45.9s\n",
      "[CV] max_depth=40, min_samples_leaf=150 ..............................\n",
      "[CV] ............... max_depth=40, min_samples_leaf=150, total=  46.6s\n",
      "[CV] max_depth=40, min_samples_leaf=150 ..............................\n",
      "[CV] ............... max_depth=40, min_samples_leaf=150, total=  45.9s\n",
      "[CV] max_depth=40, min_samples_leaf=150 ..............................\n",
      "[CV] ............... max_depth=40, min_samples_leaf=150, total=  46.5s\n",
      "[CV] max_depth=40, min_samples_leaf=150 ..............................\n",
      "[CV] ............... max_depth=40, min_samples_leaf=150, total=  45.9s\n",
      "[CV] max_depth=40, min_samples_leaf=150 ..............................\n",
      "[CV] ............... max_depth=40, min_samples_leaf=150, total=  46.0s\n",
      "[CV] max_depth=40, min_samples_leaf=150 ..............................\n",
      "[CV] ............... max_depth=40, min_samples_leaf=150, total=  46.3s\n",
      "[CV] max_depth=40, min_samples_leaf=150 ..............................\n",
      "[CV] ............... max_depth=40, min_samples_leaf=150, total=  46.4s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  40 out of  40 | elapsed: 33.7min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=10, error_score='raise',\n",
       "       estimator=RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=None,\n",
       "           max_features='auto', max_leaf_nodes=None,\n",
       "           min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "           min_samples_leaf=1, min_samples_split=2,\n",
       "           min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=1,\n",
       "           oob_score=False, random_state=99, verbose=0, warm_start=False),\n",
       "       fit_params=None, iid=True, n_jobs=1,\n",
       "       param_grid={'max_depth': [10, 40], 'min_samples_leaf': [50, 150]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring=make_scorer(haversine_scorer, greater_is_better=False),\n",
       "       verbose=2)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "n_jobs=-1 freeze the screen. Start e thereafter CPU processing is down do 0%\n",
    "I found this: https://github.com/scikit-learn/scikit-learn/issues/2889\n",
    "There seems to be no solution!\n",
    "\n",
    "Because this, n_jobs is auto (1). Take loooong time to train data.\n",
    "\"\"\"\n",
    "param_grid = {'max_depth':[10, 40],\n",
    "              'min_samples_leaf':[50, 150]}\n",
    "\n",
    "gs = GridSearchCV(RandomForestRegressor(n_estimators=100, random_state=99), param_grid=param_grid, cv=10, scoring=score_, verbose=2)\n",
    "\n",
    "gs.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-2.9978036640710832"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'max_depth': 40, 'min_samples_leaf': 50}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient boost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 4 candidates, totalling 40 fits\n",
      "[CV] estimator__min_samples_leaf=50, estimator__min_samples_split=10 .\n",
      "[CV]  estimator__min_samples_leaf=50, estimator__min_samples_split=10, total=  53.2s\n",
      "[CV] estimator__min_samples_leaf=50, estimator__min_samples_split=10 .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:   58.0s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  estimator__min_samples_leaf=50, estimator__min_samples_split=10, total=  52.9s\n",
      "[CV] estimator__min_samples_leaf=50, estimator__min_samples_split=10 .\n",
      "[CV]  estimator__min_samples_leaf=50, estimator__min_samples_split=10, total=  54.3s\n",
      "[CV] estimator__min_samples_leaf=50, estimator__min_samples_split=10 .\n",
      "[CV]  estimator__min_samples_leaf=50, estimator__min_samples_split=10, total=  51.5s\n",
      "[CV] estimator__min_samples_leaf=50, estimator__min_samples_split=10 .\n",
      "[CV]  estimator__min_samples_leaf=50, estimator__min_samples_split=10, total=  51.5s\n",
      "[CV] estimator__min_samples_leaf=50, estimator__min_samples_split=10 .\n",
      "[CV]  estimator__min_samples_leaf=50, estimator__min_samples_split=10, total=  51.5s\n",
      "[CV] estimator__min_samples_leaf=50, estimator__min_samples_split=10 .\n",
      "[CV]  estimator__min_samples_leaf=50, estimator__min_samples_split=10, total=  53.0s\n",
      "[CV] estimator__min_samples_leaf=50, estimator__min_samples_split=10 .\n",
      "[CV]  estimator__min_samples_leaf=50, estimator__min_samples_split=10, total=  53.8s\n",
      "[CV] estimator__min_samples_leaf=50, estimator__min_samples_split=10 .\n",
      "[CV]  estimator__min_samples_leaf=50, estimator__min_samples_split=10, total=  50.2s\n",
      "[CV] estimator__min_samples_leaf=50, estimator__min_samples_split=10 .\n",
      "[CV]  estimator__min_samples_leaf=50, estimator__min_samples_split=10, total=  50.1s\n",
      "[CV] estimator__min_samples_leaf=50, estimator__min_samples_split=50 .\n",
      "[CV]  estimator__min_samples_leaf=50, estimator__min_samples_split=50, total=  49.4s\n",
      "[CV] estimator__min_samples_leaf=50, estimator__min_samples_split=50 .\n",
      "[CV]  estimator__min_samples_leaf=50, estimator__min_samples_split=50, total=  49.2s\n",
      "[CV] estimator__min_samples_leaf=50, estimator__min_samples_split=50 .\n",
      "[CV]  estimator__min_samples_leaf=50, estimator__min_samples_split=50, total=  49.2s\n",
      "[CV] estimator__min_samples_leaf=50, estimator__min_samples_split=50 .\n",
      "[CV]  estimator__min_samples_leaf=50, estimator__min_samples_split=50, total=  48.6s\n",
      "[CV] estimator__min_samples_leaf=50, estimator__min_samples_split=50 .\n",
      "[CV]  estimator__min_samples_leaf=50, estimator__min_samples_split=50, total=  49.7s\n",
      "[CV] estimator__min_samples_leaf=50, estimator__min_samples_split=50 .\n",
      "[CV]  estimator__min_samples_leaf=50, estimator__min_samples_split=50, total=  49.2s\n",
      "[CV] estimator__min_samples_leaf=50, estimator__min_samples_split=50 .\n",
      "[CV]  estimator__min_samples_leaf=50, estimator__min_samples_split=50, total=  49.7s\n",
      "[CV] estimator__min_samples_leaf=50, estimator__min_samples_split=50 .\n",
      "[CV]  estimator__min_samples_leaf=50, estimator__min_samples_split=50, total=  49.1s\n",
      "[CV] estimator__min_samples_leaf=50, estimator__min_samples_split=50 .\n",
      "[CV]  estimator__min_samples_leaf=50, estimator__min_samples_split=50, total=  49.3s\n",
      "[CV] estimator__min_samples_leaf=50, estimator__min_samples_split=50 .\n",
      "[CV]  estimator__min_samples_leaf=50, estimator__min_samples_split=50, total=  49.2s\n",
      "[CV] estimator__min_samples_leaf=150, estimator__min_samples_split=10 \n",
      "[CV]  estimator__min_samples_leaf=150, estimator__min_samples_split=10, total=  46.8s\n",
      "[CV] estimator__min_samples_leaf=150, estimator__min_samples_split=10 \n",
      "[CV]  estimator__min_samples_leaf=150, estimator__min_samples_split=10, total=  47.4s\n",
      "[CV] estimator__min_samples_leaf=150, estimator__min_samples_split=10 \n",
      "[CV]  estimator__min_samples_leaf=150, estimator__min_samples_split=10, total=  46.5s\n",
      "[CV] estimator__min_samples_leaf=150, estimator__min_samples_split=10 \n",
      "[CV]  estimator__min_samples_leaf=150, estimator__min_samples_split=10, total=  46.8s\n",
      "[CV] estimator__min_samples_leaf=150, estimator__min_samples_split=10 \n",
      "[CV]  estimator__min_samples_leaf=150, estimator__min_samples_split=10, total=  47.2s\n",
      "[CV] estimator__min_samples_leaf=150, estimator__min_samples_split=10 \n",
      "[CV]  estimator__min_samples_leaf=150, estimator__min_samples_split=10, total=  48.0s\n",
      "[CV] estimator__min_samples_leaf=150, estimator__min_samples_split=10 \n",
      "[CV]  estimator__min_samples_leaf=150, estimator__min_samples_split=10, total=  47.4s\n",
      "[CV] estimator__min_samples_leaf=150, estimator__min_samples_split=10 \n",
      "[CV]  estimator__min_samples_leaf=150, estimator__min_samples_split=10, total=  47.3s\n",
      "[CV] estimator__min_samples_leaf=150, estimator__min_samples_split=10 \n",
      "[CV]  estimator__min_samples_leaf=150, estimator__min_samples_split=10, total=  47.2s\n",
      "[CV] estimator__min_samples_leaf=150, estimator__min_samples_split=10 \n",
      "[CV]  estimator__min_samples_leaf=150, estimator__min_samples_split=10, total=  47.4s\n",
      "[CV] estimator__min_samples_leaf=150, estimator__min_samples_split=50 \n",
      "[CV]  estimator__min_samples_leaf=150, estimator__min_samples_split=50, total=  47.4s\n",
      "[CV] estimator__min_samples_leaf=150, estimator__min_samples_split=50 \n",
      "[CV]  estimator__min_samples_leaf=150, estimator__min_samples_split=50, total=  50.4s\n",
      "[CV] estimator__min_samples_leaf=150, estimator__min_samples_split=50 \n",
      "[CV]  estimator__min_samples_leaf=150, estimator__min_samples_split=50, total=  48.5s\n",
      "[CV] estimator__min_samples_leaf=150, estimator__min_samples_split=50 \n",
      "[CV]  estimator__min_samples_leaf=150, estimator__min_samples_split=50, total=  48.8s\n",
      "[CV] estimator__min_samples_leaf=150, estimator__min_samples_split=50 \n",
      "[CV]  estimator__min_samples_leaf=150, estimator__min_samples_split=50, total=  48.7s\n",
      "[CV] estimator__min_samples_leaf=150, estimator__min_samples_split=50 \n",
      "[CV]  estimator__min_samples_leaf=150, estimator__min_samples_split=50, total=  48.9s\n",
      "[CV] estimator__min_samples_leaf=150, estimator__min_samples_split=50 \n",
      "[CV]  estimator__min_samples_leaf=150, estimator__min_samples_split=50, total=  48.9s\n",
      "[CV] estimator__min_samples_leaf=150, estimator__min_samples_split=50 \n",
      "[CV]  estimator__min_samples_leaf=150, estimator__min_samples_split=50, total=  49.0s\n",
      "[CV] estimator__min_samples_leaf=150, estimator__min_samples_split=50 \n",
      "[CV]  estimator__min_samples_leaf=150, estimator__min_samples_split=50, total=  48.9s\n",
      "[CV] estimator__min_samples_leaf=150, estimator__min_samples_split=50 \n",
      "[CV]  estimator__min_samples_leaf=150, estimator__min_samples_split=50, total=  49.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  40 out of  40 | elapsed: 36.1min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=10, error_score='raise',\n",
       "       estimator=MultiOutputRegressor(estimator=GradientBoostingRegressor(alpha=0.9, criterion='friedman_mse', init=None,\n",
       "             learning_rate=0.1, loss='ls', max_depth=8, max_features=None,\n",
       "             max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
       "             min_impurity_split=None, min_samples...to', random_state=99,\n",
       "             subsample=0.8, verbose=0, warm_start=False),\n",
       "           n_jobs=1),\n",
       "       fit_params=None, iid=True, n_jobs=1,\n",
       "       param_grid={'estimator__min_samples_split': [10, 50], 'estimator__min_samples_leaf': [50, 150]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring=make_scorer(haversine_scorer, greater_is_better=False),\n",
       "       verbose=2)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param_grid = {'estimator__min_samples_split':[10, 50],\n",
    "              'estimator__min_samples_leaf':[50, 150]}\n",
    "\n",
    "gb = GradientBoostingRegressor(random_state=99, learning_rate=0.1, subsample=0.8, max_depth=8)\n",
    "\n",
    "gs = GridSearchCV(MultiOutputRegressor(gb), param_grid=param_grid, cv=10, scoring=score_, verbose=2)\n",
    "\n",
    "gs.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-3.063807160105668"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'estimator__min_samples_leaf': 150, 'estimator__min_samples_split': 10}"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train all data with Random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "df = carrega_dados('train_tratado_outliers_2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>origin_call</th>\n",
       "      <th>origin_stand</th>\n",
       "      <th>taxi_id</th>\n",
       "      <th>dist_perc</th>\n",
       "      <th>start_lat</th>\n",
       "      <th>start_lon</th>\n",
       "      <th>stop_lat</th>\n",
       "      <th>stop_lon</th>\n",
       "      <th>month</th>\n",
       "      <th>day</th>\n",
       "      <th>hour</th>\n",
       "      <th>fds_fer</th>\n",
       "      <th>call_type_a</th>\n",
       "      <th>call_type_b</th>\n",
       "      <th>call_type_c</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>20000589</td>\n",
       "      <td>2.23</td>\n",
       "      <td>41.141412</td>\n",
       "      <td>-8.618643</td>\n",
       "      <td>41.154489</td>\n",
       "      <td>-8.630838</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>20000596</td>\n",
       "      <td>3.46</td>\n",
       "      <td>41.159826</td>\n",
       "      <td>-8.639847</td>\n",
       "      <td>41.170671</td>\n",
       "      <td>-8.665740</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   origin_call  origin_stand   taxi_id  dist_perc  start_lat  start_lon  \\\n",
       "0         -1.0          -1.0  20000589       2.23  41.141412  -8.618643   \n",
       "1         -1.0           7.0  20000596       3.46  41.159826  -8.639847   \n",
       "\n",
       "    stop_lat  stop_lon  month  day  hour  fds_fer  call_type_a  call_type_b  \\\n",
       "0  41.154489 -8.630838      7    1     0        0          0.0          0.0   \n",
       "1  41.170671 -8.665740      7    1     0        0          0.0          1.0   \n",
       "\n",
       "   call_type_c  \n",
       "0          1.0  \n",
       "1          0.0  "
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "# separating\n",
    "y = df[['stop_lon', 'stop_lat']]\n",
    "X = df.drop(['stop_lon', 'stop_lat'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using Min max scaler. Fit first then transform it after.\n",
    "\n",
    "# create scaler\n",
    "scaler = MinMaxScaler().fit(X)\n",
    "#transforming\n",
    "X = scaler.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tree 1 of 100\n",
      "building tree 2 of 100\n",
      "building tree 3 of 100building tree 4 of 100\n",
      "\n",
      "building tree 5 of 100\n",
      "building tree 6 of 100\n",
      "building tree 7 of 100\n",
      "building tree 8 of 100\n",
      "building tree 9 of 100\n",
      "building tree 10 of 100\n",
      "building tree 11 of 100\n",
      "building tree 12 of 100\n",
      "building tree 13 of 100\n",
      "building tree 14 of 100\n",
      "building tree 15 of 100\n",
      "building tree 16 of 100\n",
      "building tree 17 of 100\n",
      "building tree 18 of 100\n",
      "building tree 19 of 100\n",
      "building tree 20 of 100\n",
      "building tree 21 of 100\n",
      "building tree 22 of 100\n",
      "building tree 23 of 100\n",
      "building tree 24 of 100\n",
      "building tree 25 of 100\n",
      "building tree 26 of 100\n",
      "building tree 27 of 100\n",
      "building tree 28 of 100\n",
      "building tree 29 of 100\n",
      "building tree 30 of 100\n",
      "building tree 31 of 100\n",
      "building tree 32 of 100\n",
      "building tree 33 of 100\n",
      "building tree 34 of 100\n",
      "building tree 35 of 100\n",
      "building tree 36 of 100\n",
      "building tree 37 of 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  33 tasks      | elapsed:  2.5min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tree 38 of 100\n",
      "building tree 39 of 100\n",
      "building tree 40 of 100\n",
      "building tree 41 of 100\n",
      "building tree 42 of 100\n",
      "building tree 43 of 100\n",
      "building tree 44 of 100\n",
      "building tree 45 of 100\n",
      "building tree 46 of 100\n",
      "building tree 47 of 100\n",
      "building tree 48 of 100\n",
      "building tree 49 of 100\n",
      "building tree 50 of 100\n",
      "building tree 51 of 100\n",
      "building tree 52 of 100\n",
      "building tree 53 of 100\n",
      "building tree 54 of 100\n",
      "building tree 55 of 100\n",
      "building tree 56 of 100\n",
      "building tree 57 of 100\n",
      "building tree 58 of 100\n",
      "building tree 59 of 100\n",
      "building tree 60 of 100\n",
      "building tree 61 of 100\n",
      "building tree 62 of 100\n",
      "building tree 63 of 100\n",
      "building tree 64 of 100\n",
      "building tree 65 of 100\n",
      "building tree 66 of 100\n",
      "building tree 67 of 100\n",
      "building tree 68 of 100\n",
      "building tree 69 of 100\n",
      "building tree 70 of 100\n",
      "building tree 71 of 100\n",
      "building tree 72 of 100\n",
      "building tree 73 of 100\n",
      "building tree 74 of 100\n",
      "building tree 75 of 100\n",
      "building tree 76 of 100\n",
      "building tree 77 of 100\n",
      "building tree 78 of 100\n",
      "building tree 79 of 100\n",
      "building tree 80 of 100\n",
      "building tree 81 of 100\n",
      "building tree 82 of 100\n",
      "building tree 83 of 100\n",
      "building tree 84 of 100\n",
      "building tree 85 of 100\n",
      "building tree 86 of 100\n",
      "building tree 87 of 100\n",
      "building tree 88 of 100\n",
      "building tree 89 of 100\n",
      "building tree 90 of 100\n",
      "building tree 91 of 100\n",
      "building tree 92 of 100\n",
      "building tree 93 of 100\n",
      "building tree 94 of 100\n",
      "building tree 95 of 100\n",
      "building tree 96 of 100\n",
      "building tree 97 of 100\n",
      "building tree 98 of 100\n",
      "building tree 99 of 100\n",
      "building tree 100 of 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 100 out of 100 | elapsed:  7.0min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=8,\n",
       "           max_features='auto', max_leaf_nodes=None,\n",
       "           min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "           min_samples_leaf=50, min_samples_split=2,\n",
       "           min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=-1,\n",
       "           oob_score=False, random_state=99, verbose=2, warm_start=False)"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit\n",
    "# n_jobs=-1 works fine here. Seems that screen freeze have relation with custom score and how gridsearch use that.\n",
    "\n",
    "reg = RandomForestRegressor(n_estimators=100, n_jobs=-1, max_depth=8, min_samples_leaf=50, random_state=99, verbose=2)\n",
    "reg.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving pickle\n",
    "\n",
    "name = r'pickles/'+str(datetime.datetime.now().date())+'_random_forest_0.3_.save'\n",
    "pickle.dump(reg, open(name,'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading model\n",
    "\n",
    "#name_load = r'pickles/2018-09-28_random_forest_0.3_.save'\n",
    "#reg = pickle.load(open(name_load, 'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test\n",
    "\n",
    "Each one of These datasets refer to trips occurred between 01/07/2014 and 31/12/2014. Each one of these data sets will provide a snapshot of the current network status on a given timestamp. It will provide partial trajectories for each one of the on-going trips during that specific moment.\n",
    "\n",
    "The five snapshots included on the test set refer to the following timestamps:\n",
    "\n",
    "- 14/08/2014 18:00:00\n",
    "- 30/09/2014 08:30:00\n",
    "- 06/10/2014 17:45:00\n",
    "- 01/11/2014 04:00:00\n",
    "- 21/12/2014 14:30:00"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = carrega_dados('test_trated.csv', None, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>trip_id</th>\n",
       "      <th>origin_call</th>\n",
       "      <th>origin_stand</th>\n",
       "      <th>taxi_id</th>\n",
       "      <th>dist_perc</th>\n",
       "      <th>start_lat</th>\n",
       "      <th>start_lon</th>\n",
       "      <th>stop_lat</th>\n",
       "      <th>stop_lon</th>\n",
       "      <th>month</th>\n",
       "      <th>day</th>\n",
       "      <th>hour</th>\n",
       "      <th>fds_fer</th>\n",
       "      <th>call_type_a</th>\n",
       "      <th>call_type_b</th>\n",
       "      <th>call_type_c</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>T1</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>20000542</td>\n",
       "      <td>0.51</td>\n",
       "      <td>41.148522</td>\n",
       "      <td>-8.585676</td>\n",
       "      <td>41.146623</td>\n",
       "      <td>-8.584884</td>\n",
       "      <td>8</td>\n",
       "      <td>14</td>\n",
       "      <td>17</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>T2</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>57.0</td>\n",
       "      <td>20000108</td>\n",
       "      <td>2.28</td>\n",
       "      <td>41.145570</td>\n",
       "      <td>-8.610876</td>\n",
       "      <td>41.163597</td>\n",
       "      <td>-8.601894</td>\n",
       "      <td>8</td>\n",
       "      <td>14</td>\n",
       "      <td>17</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>T3</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>20000370</td>\n",
       "      <td>3.91</td>\n",
       "      <td>41.148558</td>\n",
       "      <td>-8.585739</td>\n",
       "      <td>41.167719</td>\n",
       "      <td>-8.574903</td>\n",
       "      <td>8</td>\n",
       "      <td>14</td>\n",
       "      <td>17</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>T4</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>53.0</td>\n",
       "      <td>20000492</td>\n",
       "      <td>0.18</td>\n",
       "      <td>41.141169</td>\n",
       "      <td>-8.613963</td>\n",
       "      <td>41.140980</td>\n",
       "      <td>-8.614638</td>\n",
       "      <td>8</td>\n",
       "      <td>14</td>\n",
       "      <td>17</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>T5</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>20000621</td>\n",
       "      <td>0.00</td>\n",
       "      <td>41.148036</td>\n",
       "      <td>-8.619903</td>\n",
       "      <td>41.148036</td>\n",
       "      <td>-8.619894</td>\n",
       "      <td>8</td>\n",
       "      <td>14</td>\n",
       "      <td>17</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  trip_id  origin_call  origin_stand   taxi_id  dist_perc  start_lat  \\\n",
       "0      T1         -1.0          15.0  20000542       0.51  41.148522   \n",
       "1      T2         -1.0          57.0  20000108       2.28  41.145570   \n",
       "2      T3         -1.0          15.0  20000370       3.91  41.148558   \n",
       "3      T4         -1.0          53.0  20000492       0.18  41.141169   \n",
       "4      T5         -1.0          18.0  20000621       0.00  41.148036   \n",
       "\n",
       "   start_lon   stop_lat  stop_lon  month  day  hour  fds_fer  call_type_a  \\\n",
       "0  -8.585676  41.146623 -8.584884      8   14    17        0          0.0   \n",
       "1  -8.610876  41.163597 -8.601894      8   14    17        0          0.0   \n",
       "2  -8.585739  41.167719 -8.574903      8   14    17        0          0.0   \n",
       "3  -8.613963  41.140980 -8.614638      8   14    17        0          0.0   \n",
       "4  -8.619903  41.148036 -8.619894      8   14    17        0          0.0   \n",
       "\n",
       "   call_type_b  call_type_c  \n",
       "0          1.0          0.0  \n",
       "1          1.0          0.0  \n",
       "2          1.0          0.0  \n",
       "3          1.0          0.0  \n",
       "4          1.0          0.0  "
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.set_index('trip_id', inplace=True)\n",
    "y2 = df_test[['stop_lon', 'stop_lat']]\n",
    "X2 = df_test.drop(['stop_lon', 'stop_lat'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "X2_scaled = scaler.transform(X2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Done  33 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done 100 out of 100 | elapsed:    0.0s finished\n"
     ]
    }
   ],
   "source": [
    "preds = reg.predict(X2_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Done  33 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done 100 out of 100 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-2.558431034608694"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score_(reg, X2_scaled, y2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model fulfilled its role and put itself well with new data achieving to generalize new points with good performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ploting results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Essa  a mesma formula usada para fazer o scorer.\n",
    "# Aqui esta mais legvel e computa apenas a distancia\n",
    "# entre os pontos.\n",
    "\n",
    "def haversine(lon1, lat1, lon2, lat2):\n",
    "    \"\"\"\n",
    "    Calculate the great circle distance between two points \n",
    "    on the earth (specified in decimal degrees)\n",
    "    \"\"\"\n",
    "    # convert decimal degrees to radians \n",
    "    lon1, lat1, lon2, lat2 = map(radians, [lon1, lat1, lon2, lat2])\n",
    "\n",
    "    a = sin((lat2 - lat1)/2)**2 + cos(lat1) * cos(lat2) * sin((lon2 - lon1)/2)**2\n",
    "    c = 2 * asin(sqrt(a)) \n",
    "    r = 6371 # Radius of earth in kilometers. Use 3956 for miles\n",
    "    return c * r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = y2.copy()\n",
    "result['pred_stop_lat'] = preds[:,0]\n",
    "result['pred_stop_lon'] = preds[:,1]\n",
    "result['diff_dist'] = result.apply(lambda x: haversine(x[0], x[1], x[2], x[3]), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "result = pd.concat([result, X2[['start_lon', 'start_lat']]], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>stop_lon</th>\n",
       "      <th>stop_lat</th>\n",
       "      <th>pred_stop_lat</th>\n",
       "      <th>pred_stop_lon</th>\n",
       "      <th>diff_dist</th>\n",
       "      <th>start_lon</th>\n",
       "      <th>start_lat</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>trip_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>T1</th>\n",
       "      <td>-8.584884</td>\n",
       "      <td>41.146623</td>\n",
       "      <td>-8.604468</td>\n",
       "      <td>41.156994</td>\n",
       "      <td>2.004555</td>\n",
       "      <td>-8.585676</td>\n",
       "      <td>41.148522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>T2</th>\n",
       "      <td>-8.601894</td>\n",
       "      <td>41.163597</td>\n",
       "      <td>-8.612704</td>\n",
       "      <td>41.153483</td>\n",
       "      <td>1.443557</td>\n",
       "      <td>-8.610876</td>\n",
       "      <td>41.145570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>T3</th>\n",
       "      <td>-8.574903</td>\n",
       "      <td>41.167719</td>\n",
       "      <td>-8.604984</td>\n",
       "      <td>41.157335</td>\n",
       "      <td>2.770293</td>\n",
       "      <td>-8.585739</td>\n",
       "      <td>41.148558</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>T4</th>\n",
       "      <td>-8.614638</td>\n",
       "      <td>41.140980</td>\n",
       "      <td>-8.612704</td>\n",
       "      <td>41.153483</td>\n",
       "      <td>1.399627</td>\n",
       "      <td>-8.613963</td>\n",
       "      <td>41.141169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>T5</th>\n",
       "      <td>-8.619894</td>\n",
       "      <td>41.148036</td>\n",
       "      <td>-8.621989</td>\n",
       "      <td>41.155700</td>\n",
       "      <td>0.870028</td>\n",
       "      <td>-8.619903</td>\n",
       "      <td>41.148036</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         stop_lon   stop_lat  pred_stop_lat  pred_stop_lon  diff_dist  \\\n",
       "trip_id                                                                 \n",
       "T1      -8.584884  41.146623      -8.604468      41.156994   2.004555   \n",
       "T2      -8.601894  41.163597      -8.612704      41.153483   1.443557   \n",
       "T3      -8.574903  41.167719      -8.604984      41.157335   2.770293   \n",
       "T4      -8.614638  41.140980      -8.612704      41.153483   1.399627   \n",
       "T5      -8.619894  41.148036      -8.621989      41.155700   0.870028   \n",
       "\n",
       "         start_lon  start_lat  \n",
       "trip_id                        \n",
       "T1       -8.585676  41.148522  \n",
       "T2       -8.610876  41.145570  \n",
       "T3       -8.585739  41.148558  \n",
       "T4       -8.613963  41.141169  \n",
       "T5       -8.619903  41.148036  "
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
